import json, os, re, tiktoken, time
import pandas as pd
from dotenv import load_dotenv
from google import genai
from google.genai import types

# 1. è¼‰å…¥ .env æª”æ¡ˆä¸¦åˆå§‹åŒ– Client
load_dotenv()
api_key = os.getenv("GEMINI_API_KEY")

if not api_key:
    raise ValueError("æ‰¾ä¸åˆ° GEMINI_API_KEYã€‚è«‹ç¢ºä¿ .env æª”æ¡ˆå­˜åœ¨ä¸”è¨­å®šæ­£ç¢ºã€‚")

client = genai.Client(api_key=api_key)

# ===== Token estimation utilities =====
enc = tiktoken.get_encoding("cl100k_base")

def estimate_tokens(text: str) -> int:
    if not text:
        return 0
    return len(enc.encode(text))


def process_esg_news_verification(input_json_path, news_json_path, msci_json_path, output_json_path):
    """
    è™•ç† ESG æ–°èé©—è­‰
    
    Args:
        input_json_path: åŸæª”è·¯å¾‘ (2024_1102_p1_keyword.json)
        news_json_path: é©—è­‰è³‡æ–™è·¯å¾‘ (2024_1102_news_results.json)
        msci_json_path: MSCI åˆ¤æ–·æ¨™æº–è·¯å¾‘ (msci_flag.json)
        output_json_path: è¼¸å‡ºçµæœè·¯å¾‘
    """
    total_start_time = time.perf_counter()
    
    # 2. è®€å–åŸæª”
    try:
        with open(input_json_path, 'r', encoding='utf-8') as f:
            original_data = json.load(f)
        print(f"âœ… æˆåŠŸè®€å–åŸæª”ï¼š{len(original_data)} ç­†è³‡æ–™")
    except FileNotFoundError:
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°è¼¸å…¥æª”æ¡ˆ {input_json_path}")
        return
    except json.JSONDecodeError:
        print(f"âŒ éŒ¯èª¤ï¼šè¼¸å…¥æª”æ¡ˆ {input_json_path} æ ¼å¼ä¸¦éæ­£ç¢ºçš„ JSON")
        return

    # 3. ä½¿ç”¨ pandas è®€å–é©—è­‰è³‡æ–™
    try:
        news_df = pd.read_json(news_json_path, encoding='utf-8')
        print(f"âœ… æˆåŠŸè®€å–é©—è­‰è³‡æ–™ï¼š{len(news_df)} ç­†æ–°è")
    except Exception as e:
        print(f"âŒ éŒ¯èª¤ï¼šè®€å–é©—è­‰è³‡æ–™å¤±æ•— - {e}")
        return

    # 4. ä½¿ç”¨ pandas è®€å– MSCI åˆ¤æ–·æ¨™æº–
    try:
        with open(msci_json_path, 'r', encoding='utf-8') as f:
            msci_flag = json.load(f)
        print(f"âœ… æˆåŠŸè®€å– MSCI åˆ¤æ–·æ¨™æº–")
    except Exception as e:
        print(f"âŒ éŒ¯èª¤ï¼šè®€å– MSCI æ¨™æº–å¤±æ•— - {e}")
        return

    # 5. æº–å‚™ Promptï¼ˆå°‡è®Šæ•¸åµŒå…¥ï¼‰
    prompt_template = f"""
ä½ å°‡æ‰®æ¼”ESGå¯©æŸ¥å“¡ï¼Œè² è²¬é€²è¡Œå¤–éƒ¨æ–°èæ¯”å°èˆ‡é¢¨éšªèª¿æ•´ã€‚

ã€åŸæª”èªªæ˜ã€‘
åŸæª”ç‚ºè©²å…¬å¸æ°¸çºŒå ±å‘Šæ›¸çš„è²æ˜èˆ‡é¢¨éšªåˆ†æ•¸ï¼ŒåŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š
- company: è‚¡ç¥¨ä»£è™Ÿ
- year: å¹´åˆ†
- esg_category: ESGåˆ†é¡ (E/S/G)
- sasb_topic: SASBä¸»é¡Œ
- page_number: é ç¢¼
- report_claim: ä¼æ¥­è²æ˜
- greenwashing_factor: æ¼‚ç¶ é¢¨éšªå› å­
- risk_score: é¢¨éšªåˆ†æ•¸
- key_word: é—œéµå­—

ã€é©—è­‰è³‡æ–™èªªæ˜ã€‘
é©—è­‰è³‡æ–™åŒ…å« {len(news_df)} ç­†æ–°èï¼Œæ¬„ä½å¦‚ä¸‹ï¼š
- news_id: æ–°èç·¨è™Ÿ
- stock_code: è‚¡ç¥¨ä»£è™Ÿ
- company_name: å…¬å¸åç¨±
- sasb_topic: SASBä¸»é¡Œ
- search_query: æœå°‹é—œéµå­—
- title: æ–°èæ¨™é¡Œ
- url: æ–°èé€£çµ
- published_date: ç™¼å¸ƒæ—¥æœŸ
- publisher: ç™¼å¸ƒè€…

ã€MSCI é¢¨éšªæ——è™Ÿåˆ¤æ–·æ¨™æº–ã€‘
{json.dumps(msci_flag, ensure_ascii=False, indent=2)}

ã€è™•ç†é‚è¼¯ã€‘
1. é¢¨éšªèª¿æ•´é‚è¼¯ä¾ç…§ä¸Šè¿° MSCI æ¨™æº–
2. ä½¿ç”¨éˆå¼æ€è€ƒï¼Œå…ˆåˆ¤æ–·ã€Œå—å½±éŸ¿äººæ•¸ã€ã€ã€Œæ˜¯å¦æ¶‰åŠæ­»äº¡ã€ã€ã€Œæ˜¯å¦é•åæ³•è¦ã€ï¼Œæœ€å¾Œå†è¼¸å‡ºæ——è™Ÿ
3. æ‰£åˆ†æ©Ÿåˆ¶ï¼šred = -4, orange = -2, yellow = -1, green = 0
4. ç‰¹åˆ¥æ³¨æ„ã€Œæ©˜æ——ã€èˆ‡ã€Œç´…æ——ã€çš„é‚Šç•Œï¼š
   - Red é€šå¸¸æ¶‰åŠã€Œç³»çµ±æ€§ã€é•·æœŸã€ä¸å¯é€†ã€
   - Orange å‰‡å¤šç‚ºã€Œå¤§è¦æ¨¡ã€åš´é‡ã€ä½†å·²é–‹å§‹ä¿®å¾©ã€
5. å…ˆæ¯”å° sasb_topic ä¸€è‡´ï¼Œå†ä¾æ“šåŸæª” report_claim å¾é©—è­‰è³‡æ–™é¸å‡ºä¸€ç­†æœ€å…·ä»£è¡¨æ€§çš„æ–°è
6. è‹¥åŸæª”è¼¸å…¥ X ç­†è²ç¨±ï¼Œå°±è¦è¼¸å‡º X ç­†çµæœ

ã€ç›¸é—œæ€§æª¢æŸ¥ã€‘
æ¯”å°å‰ï¼Œè«‹å…ˆåŸ·è¡Œç›¸é—œæ€§æª¢æŸ¥ï¼š
- æª¢æŸ¥é©—è­‰è³‡æ–™æ˜¯å¦æ˜ç¢ºæåŠ 'company' æˆ– 'company_code'
- å¦‚æœæ˜¯åœ¨è¬›å…¶ä»–å…¬å¸ï¼Œè«‹åˆ¤å®šç‚ºç„¡æ•ˆ
- æª¢æŸ¥æ–°èå…§å®¹æ˜¯å¦èˆ‡ report_claim çš„ä¸»é¡Œæœ‰å¯¦è³ªé—œè¯ï¼Ÿ
- å¦‚æœç™¼ç¾æ–°èèˆ‡å…¬å¸ç„¡é—œã€ä¸»é¡Œå®Œå…¨ä¸ç¬¦ã€ç„¡æ–°èï¼Œè«‹ç›´æ¥è¼¸å‡ºï¼š
  * consistency_status: "ä¸€è‡´"
  * external_evidence: "ç„¡ç›¸é—œæ–°èè­‰æ“š"
  * external_evidence_url: ""
  * MSCI_flag: "Green"
  * adjustment_score: (ç¶­æŒåŸ risk_score)

ã€è¼¸å‡ºæ ¼å¼ã€‘
è¼¸å‡ºæ¬„ä½è¦æ±‚ (åš´æ ¼åŸ·è¡Œ)ï¼Œä¸è¦æ·»åŠ ä»»ä½•å‰è¨€ã€å¾Œèªæˆ–èªªæ˜æ–‡å­—ã€‚ï¼š
**company**: {original_data[0]['company']},
**year**: {original_data[0]['year']},
**esg_category**: {original_data[0]['esg_category']},
**sasb_topic**: {original_data[0]['sasb_topic']},
**page_number**: {original_data[0]['page_number']},
**report_claim**: {original_data[0]['report_claim']},
**greenwashing_factor**: {original_data[0]['greenwashing_factor']},
**risk_score**: {original_data[0]['risk_score']},
**external_evidence**: é©—è­‰è³‡æ–™æ¨™é¡Œæˆ–'ç„¡ç›¸é—œæ–°èè­‰æ“š',
**external_evidence_url**: é©—è­‰è³‡æ–™æ–°èé€£çµæˆ–ç©ºå­—ä¸²,
**consistency_status**: ä¸€è‡´/éƒ¨åˆ†ä¸€è‡´/éƒ¨åˆ†ä¸€è‡´/ä¸ä¸€è‡´(å°æ‡‰MSCI_flag),
**MSCI_flag**: Green/Yellow/Orange/Red,
**adjustment_score**: "èª¿æ•´å¾Œåˆ†æ•¸ï¼ˆæœ€ä½ç‚º0ï¼‰"


çµ•å°ä¸è¦å¼·è¡Œå°‡ç„¡é—œçš„æ–°èé€£çµåˆ°ä¼æ¥­è²ç¨±ä¸Šã€‚
è«‹ç›´æ¥è¼¸å‡º JSON Arrayã€‚
"""

    # 6. å°‡åŸæª”å’Œé©—è­‰è³‡æ–™è½‰ç‚ºå­—ä¸²
    user_input = f"""
    ã€åŸæª”æ•¸æ“šã€‘
    {json.dumps(original_data, ensure_ascii=False, indent=2)}

    ã€é©—è­‰è³‡æ–™ã€‘
    {news_df.to_json(orient='records', force_ascii=False, indent=2)}
    """

    # ===== Token count (input) =====
    input_token_est = estimate_tokens(prompt_template + user_input)
    print(f"\nğŸ“Š ä¼°è¨ˆè¼¸å…¥ Token æ•¸ï¼š{input_token_est:,}")

    # 7. å‘¼å« Gemini APIï¼ˆå•Ÿç”¨ grounding with google searchï¼‰
    print("\nğŸ”„ æ­£åœ¨å‘¼å« Gemini API ä¸¦æª¢ç´¢å¤–éƒ¨è³‡è¨Šï¼Œè«‹ç¨å€™...")
    api_start_time = time.perf_counter()

    try:
        response = client.models.generate_content(
            model="gemini-2.5-pro",
            contents=user_input,
            config=types.GenerateContentConfig(
                system_instruction=prompt_template,
                temperature=0,
                response_mime_type="application/json"
                # tools=[types.Tool(google_search=types.GoogleSearch())],
            )
        )
    except Exception as e:
        print(f"âŒ API å‘¼å«å¤±æ•—: {e}")
        return

    api_end_time = time.perf_counter()
    api_elapsed = api_end_time - api_start_time
    print(f"âœ… Gemini API å‘¼å«å®Œæˆï¼Œè€—æ™‚ {api_elapsed:.2f} ç§’")

    # 8. è™•ç†èˆ‡å„²å­˜çµæœ
    raw_text = response.text.strip()

    # ===== Token count (output) =====
    output_token_est = estimate_tokens(raw_text)
    total_token_est = input_token_est + output_token_est

    # é¡¯ç¤ºåŸå§‹å›æ‡‰å‰ 500 å­—å…ƒç”¨æ–¼èª¿è©¦
    print(f"\nğŸ“„ API åŸå§‹å›æ‡‰ï¼ˆå‰ 500 å­—å…ƒï¼‰ï¼š\n{raw_text[:500]}\n")
    
    try:
        final_json = None
        
        # æ–¹æ³• 1: æª¢æ¸¬ä¸¦ç§»é™¤ markdown ä»£ç¢¼å¡Šæ¨™è¨˜ï¼ˆå„ªå…ˆï¼‰
        if raw_text.startswith("```json") or raw_text.startswith("```"):
            print("ğŸ” æª¢æ¸¬åˆ° markdown ä»£ç¢¼å¡Šæ ¼å¼ï¼Œæ­£åœ¨ç§»é™¤æ¨™è¨˜...")
            # ç§»é™¤é–‹é ­çš„ ```json æˆ– ```
            clean_text = re.sub(r'^```(?:json)?\s*\n?', '', raw_text)
            # ç§»é™¤çµå°¾çš„ ```
            clean_text = re.sub(r'\n?```\s*$', '', clean_text)
            
            try:
                final_json = json.loads(clean_text.strip())
                print("âœ… ä½¿ç”¨æ–¹æ³• 1ï¼ˆç§»é™¤ markdown æ¨™è¨˜ï¼‰æˆåŠŸè§£æ")
            except json.JSONDecodeError as e:
                print(f"âš ï¸  æ–¹æ³• 1 å¤±æ•—: {e}")
        
        # æ–¹æ³• 2: ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æå– JSON ä»£ç¢¼å¡Š
        if not final_json and "```" in raw_text:
            print("ğŸ” å˜—è©¦ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æå– JSON...")
            json_match = re.search(r'```(?:json)?\s*(\[.*?\])\s*```', raw_text, re.DOTALL)
            if json_match:
                try:
                    final_json = json.loads(json_match.group(1))
                    print("âœ… ä½¿ç”¨æ–¹æ³• 2ï¼ˆæ­£å‰‡æå– markdownï¼‰æˆåŠŸè§£æ")
                except json.JSONDecodeError as e:
                    print(f"âš ï¸  æ–¹æ³• 2 å¤±æ•—: {e}")
        
        # æ–¹æ³• 3: ç›´æ¥æŸ¥æ‰¾ JSON é™£åˆ—ï¼ˆç„¡ markdown æ¨™è¨˜ï¼‰
        if not final_json:
            print("ğŸ” å˜—è©¦ç›´æ¥æŸ¥æ‰¾ JSON é™£åˆ—...")
            all_arrays = re.findall(r'(\[.*\])', raw_text, re.DOTALL)
            if all_arrays:
                clean_json_str = all_arrays[0]
                # è™•ç†å¯èƒ½çš„å¤šå€‹é™£åˆ—
                if "][" in clean_json_str:
                    clean_json_str = clean_json_str.split("][")[0] + "]"
                elif "] [" in clean_json_str:
                    clean_json_str = clean_json_str.split("] [")[0] + "]"
                
                try:
                    final_json = json.loads(clean_json_str)
                    print("âœ… ä½¿ç”¨æ–¹æ³• 3ï¼ˆç›´æ¥æå–é™£åˆ—ï¼‰æˆåŠŸè§£æ")
                except json.JSONDecodeError as e:
                    print(f"âš ï¸  æ–¹æ³• 3 å¤±æ•—: {e}")
        
        # å„²å­˜çµæœ
        if final_json:
            os.makedirs(os.path.dirname(output_json_path), exist_ok=True)
            with open(output_json_path, 'w', encoding='utf-8') as f:
                json.dump(final_json, f, ensure_ascii=False, indent=2)
            print(f"âœ… æˆåŠŸï¼çµæœå·²å„²å­˜è‡³ {output_json_path}ï¼Œå…± {len(final_json)} ç­†")
        else:
            raise ValueError("ç„¡æ³•å¾å›æ‡‰ä¸­æå– JSON çµæ§‹")

    except json.JSONDecodeError as e:
        print(f"âš ï¸  JSON è§£æéŒ¯èª¤: {e}")
        print("âš ï¸  æ­£åœ¨å˜—è©¦ä¿®å¾©æ¨¡å¼...")
        try:
            # ä½¿ç”¨æ‹¬è™Ÿè¨ˆæ•¸æ³•æå–å®Œæ•´ JSON
            start = raw_text.find('[')
            if start == -1:
                raise ValueError("æ‰¾ä¸åˆ° JSON é™£åˆ—èµ·å§‹æ¨™è¨˜ '['")
            
            count = 0
            end_pos = -1
            for i in range(start, len(raw_text)):
                if raw_text[i] == '[':
                    count += 1
                elif raw_text[i] == ']':
                    count -= 1
                if count == 0:
                    end_pos = i + 1
                    break
            
            if end_pos > start:
                extreme_clean = raw_text[start:end_pos]
                final_json = json.loads(extreme_clean)
                os.makedirs(os.path.dirname(output_json_path), exist_ok=True)
                with open(output_json_path, 'w', encoding='utf-8') as f:
                    json.dump(final_json, f, ensure_ascii=False, indent=2)
                print(f"âœ… ä¿®å¾©æˆåŠŸï¼çµæœå„²å­˜è‡³ {output_json_path}ï¼Œå…± {len(final_json)} ç­†")
            else:
                raise ValueError("ç„¡æ³•æ‰¾åˆ°å®Œæ•´çš„ JSON é™£åˆ—")
        except Exception as repair_error:
            print(f"âŒ ä¿®å¾©å¤±æ•—ï¼š{repair_error}")
            # å°‡åŸå§‹å›æ‡‰å„²å­˜åˆ°æ–‡ä»¶ä»¥ä¾¿èª¿è©¦
            debug_path = output_json_path.replace('.json', '_debug_response.txt')
            with open(debug_path, 'w', encoding='utf-8') as f:
                f.write(raw_text)
            print(f"ğŸ’¾ å·²å°‡å®Œæ•´åŸå§‹å›æ‡‰å„²å­˜è‡³ï¼š{debug_path}")
    except Exception as e:
        print(f"âŒ ç™¼ç”Ÿéé æœŸéŒ¯èª¤: {e}")
        # å°‡åŸå§‹å›æ‡‰å„²å­˜åˆ°æ–‡ä»¶ä»¥ä¾¿èª¿è©¦
        debug_path = output_json_path.replace('.json', '_debug_response.txt')
        with open(debug_path, 'w', encoding='utf-8') as f:
            f.write(raw_text)
        print(f"ğŸ’¾ å·²å°‡å®Œæ•´åŸå§‹å›æ‡‰å„²å­˜è‡³ï¼š{debug_path}")

    # ===== TOKEN USAGE & TIME COST =====
    total_end_time = time.perf_counter()
    total_elapsed = total_end_time - total_start_time

    print("\n" + "="*50)
    print("ğŸ“Š Token ä½¿ç”¨çµ±è¨ˆ")
    print("="*50)
    print(f"è¼¸å…¥ Token æ•¸ : {input_token_est:,}")
    print(f"è¼¸å‡º Token æ•¸ : {output_token_est:,}")
    print(f"ç¸½è¨ˆ Token æ•¸ : {total_token_est:,}")
    print("\n" + "="*50)
    print("â±ï¸  åŸ·è¡Œæ™‚é–“çµ±è¨ˆ")
    print("="*50)
    print(f"API å‘¼å«æ™‚é–“  : {api_elapsed:.2f} ç§’")
    print(f"ç¸½åŸ·è¡Œæ™‚é–“    : {total_elapsed:.2f} ç§’")
    print("="*50)


if __name__ == "__main__":
    # è¨­å®šæª”æ¡ˆè·¯å¾‘
    input_path = './temp_data/prompt1_json/2024_1102_p1_keyword.json'
    news_path = './news_search/news_output/2024_1102_news_results.json'
    msci_path = './static/data/msci_flag.json'
    output_path = './temp_data/prompt2_json/2024_1102_p2.json'
    
    process_esg_news_verification(input_path, news_path, msci_path, output_path)
