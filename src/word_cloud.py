"""
Word Cloud æ–‡å­—é›²ç”Ÿæˆæ¨¡çµ„

æä¾›å¾ ESG å ±å‘Šæ›¸ PDF ç”Ÿæˆæ–‡å­—é›² JSON çš„åŠŸèƒ½ã€‚

ä¸»è¦å‡½æ•¸ï¼š
    generate_wordcloud: ç”Ÿæˆæ–‡å­—é›² JSON æª”æ¡ˆ

ä½¿ç”¨ç¯„ä¾‹ï¼š
    # åŸºæœ¬ä½¿ç”¨
    from word_cloud.word_cloud import generate_wordcloud
    
    result = generate_wordcloud(year=2024, company_code="1102")
    if result['success']:
        print(f"ç”ŸæˆæˆåŠŸ: {result['output_file']}")
"""

import jieba
import os
import sys
from collections import Counter
import pdfplumber
import time
import json
import glob
from typing import Dict, List, Optional

# å°å…¥é›†ä¸­é…ç½®
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from config import PATHS, DATA_FILES

# æ¨¡çµ„å¸¸æ•¸ - ä½¿ç”¨ config.py çš„è·¯å¾‘å®šç¾©
DICT_DIR = PATHS['STATIC_DICT']  # å­—å…¸æª”ç›®éŒ„
OUTPUT_DIR = PATHS['WORD_CLOUD_OUTPUT']  # æ–‡å­—é›²è¼¸å‡ºç›®éŒ„
PDF_DIR = PATHS['ESG_REPORTS']  # PDF å ±å‘Šæ›¸ç›®éŒ„


def _extract_text_from_pdf(pdf_path: str) -> str:
    """
    è®€å– PDF ä¸¦æå–æ–‡å­—
    
    Args:
        pdf_path: PDF æª”æ¡ˆè·¯å¾‘
    
    Returns:
        str: æå–çš„æ–‡å­—å…§å®¹
    """
    print(f"æ­£åœ¨è®€å– PDF: {pdf_path} ...")
    text = ""
    try:
        with pdfplumber.open(pdf_path) as pdf:
            for i, page in enumerate(pdf.pages):
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
                if (i + 1) % 10 == 0:
                    print(f"  å·²è™•ç† {i + 1} é ...")
        print("PDF è®€å–å®Œæˆã€‚")
        return text
    except Exception as e:
        print(f"PDF è®€å–å¤±æ•—: {e}")
        return ""


def _load_dictionaries() -> None:
    """è¼‰å…¥è‡ªè¨‚è©å…¸"""
    try:
        for filename in ["esg_dict.txt", "fuzzy_dict.txt"]:
            full_path = os.path.join(DICT_DIR, filename)
            if os.path.exists(full_path):
                jieba.load_userdict(full_path)
    except Exception as e:
        print(f"æé†’ï¼šå­—å…¸æª”è®€å–å¤±æ•— ({e})ï¼Œå°‡åƒ…ä½¿ç”¨é è¨­æ–·è©ã€‚")


def _load_stopwords() -> set:
    """
    è¼‰å…¥åœç”¨è©
    
    Returns:
        set: åœç”¨è©é›†åˆ
    """
    stopwords_path = os.path.join(DICT_DIR, "stopword_list.txt")
    try:
        with open(stopwords_path, 'r', encoding='utf-8') as f:
            return set(f.read().splitlines())
    except Exception as e:
        print(f"è­¦å‘Šï¼šåœç”¨è©æª”è®€å–å¤±æ•— ({e})ï¼Œå°‡ä½¿ç”¨ç©ºé›†åˆã€‚")
        return set()


def generate_wordcloud(
    year: int,
    company_code: str,
    pdf_path: Optional[str] = None,
    force_regenerate: bool = False
) -> Dict:
    """
    ç”Ÿæˆ ESG å ±å‘Šæ›¸çš„æ–‡å­—é›² JSON
    
    Args:
        year: å ±å‘Šå¹´ä»½
        company_code: å…¬å¸ä»£ç¢¼
        pdf_path: PDF æª”æ¡ˆè·¯å¾‘ï¼ˆé¸å¡«ï¼Œè‹¥æœªæä¾›å‰‡è‡ªå‹•æœå°‹ï¼‰
        force_regenerate: æ˜¯å¦å¼·åˆ¶é‡æ–°ç”Ÿæˆï¼ˆé è¨­ Falseï¼Œæœƒæª¢æŸ¥æª”æ¡ˆæ˜¯å¦å·²å­˜åœ¨ï¼‰
    
    Returns:
        dict: {
            'success': bool,           # æ˜¯å¦æˆåŠŸ
            'output_file': str,        # JSON æª”æ¡ˆè·¯å¾‘
            'word_count': int,         # é—œéµå­—æ•¸é‡
            'top_keywords': list,      # å‰ 10 å€‹é—œéµå­—
            'skipped': bool,           # æ˜¯å¦å› å·²å­˜åœ¨è€Œè·³é
            'error': str               # éŒ¯èª¤è¨Šæ¯ï¼ˆè‹¥æœ‰ï¼‰
        }
    """
    start_time = time.time()
    
    # === 1. å»ºç«‹è¼¸å‡ºè·¯å¾‘ ===
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    output_filename = f"{year}_{company_code}_wc.json"
    output_path = os.path.join(OUTPUT_DIR, output_filename)
    
    # === 2. æª”æ¡ˆå­˜åœ¨æ€§æª¢æŸ¥ ===
    if not force_regenerate and os.path.exists(output_path):
        try:
            with open(output_path, 'r', encoding='utf-8') as f:
                existing_data = json.load(f)
            
            # é©—è­‰æ ¼å¼æ­£ç¢º
            if isinstance(existing_data, list) and len(existing_data) > 0:
                if all('name' in item and 'value' in item for item in existing_data[:5]):
                    top_10 = existing_data[:10] if len(existing_data) >= 10 else existing_data
                    
                    execution_time = time.time() - start_time
                    print(f"â„¹ï¸ æ–‡å­—é›² JSON å·²å­˜åœ¨ï¼Œè·³éç”Ÿæˆ (è€—æ™‚: {execution_time:.2f} ç§’)")
                    
                    return {
                        'success': True,
                        'output_file': output_path,
                        'word_count': len(existing_data),
                        'top_keywords': [item['name'] for item in top_10],
                        'skipped': True
                    }
        except (json.JSONDecodeError, KeyError, IOError, TypeError) as e:
            print(f"âš ï¸ ç¾æœ‰æª”æ¡ˆæ ¼å¼éŒ¯èª¤ ({e})ï¼Œå°‡é‡æ–°ç”Ÿæˆ")
    
    # === 3. å°‹æ‰¾ PDF æª”æ¡ˆ ===
    if pdf_path is None:
        pattern = os.path.join(PDF_DIR, f"{year}_{company_code}_*.pdf")
        matched_files = glob.glob(pattern)
        
        if not matched_files:
            return {
                'success': False,
                'error': f"æ‰¾ä¸åˆ°ç¬¦åˆæ ¼å¼çš„ PDF æª”æ¡ˆ: {pattern}"
            }
        elif len(matched_files) > 1:
            print(f"âš ï¸ æ‰¾åˆ°å¤šå€‹ç¬¦åˆçš„æª”æ¡ˆï¼Œå°‡ä½¿ç”¨ç¬¬ä¸€å€‹: {matched_files[0]}")
            pdf_path = matched_files[0]
        else:
            pdf_path = matched_files[0]
            print(f"æ‰¾åˆ°æª”æ¡ˆ: {pdf_path}")
    
    # === 4. æå–æ–‡å­— ===
    text = _extract_text_from_pdf(pdf_path)
    if not text:
        return {
            'success': False,
            'error': 'PDF æ–‡å­—æå–å¤±æ•—'
        }
    
    # === 5. è¼‰å…¥è©å…¸å’Œåœç”¨è© ===
    _load_dictionaries()
    stopwords = _load_stopwords()
    
    # === 6. æ–·è©ä¸¦éæ¿¾ ===
    words = jieba.lcut(text)
    filtered_words = [
        w for w in words
        if len(w) >= 2 and w != '\n' and w not in stopwords
    ]
    
    # === 7. è¨ˆç®—å­—é » ===
    word_counts = Counter(filtered_words)
    
    # === 8. ç”Ÿæˆ JSON ===
    word_cloud_json = [
        {"name": word, "value": count}
        for word, count in word_counts.most_common(100)
    ]
    
    # === 9. å„²å­˜æª”æ¡ˆ ===
    try:
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(word_cloud_json, f, ensure_ascii=False, indent=2)
        
        top_10 = word_cloud_json[:10] if len(word_cloud_json) >= 10 else word_cloud_json
        execution_time = time.time() - start_time
        
        print(f"âœ… JSON æª”å·²å„²å­˜è‡³: {output_path}")
        print(f"â±ï¸ ç¨‹å¼åŸ·è¡Œæ™‚é–“: {execution_time:.2f} ç§’")
        
        return {
            'success': True,
            'output_file': output_path,
            'word_count': len(word_cloud_json),
            'top_keywords': [item['name'] for item in top_10],
            'skipped': False
        }
    
    except Exception as e:
        return {
            'success': False,
            'error': f'å„²å­˜ JSON å¤±æ•—: {str(e)}'
        }


# =========================
# å‘½ä»¤åˆ—åŸ·è¡Œå…¥å£
# =========================

def main():
    """å‘½ä»¤åˆ—åŸ·è¡Œçš„ä¸»å‡½æ•¸"""
    print("=== ESG å ±å‘Šæ›¸æ–‡å­—é›²ç”Ÿæˆå™¨ ===\n")
    
    year = input("è«‹è¼¸å…¥å¹´ä»½ (é è¨­ 2024): ").strip() or "2024"
    company_code = input("è«‹è¼¸å…¥å…¬å¸ä»£ç¢¼ (é è¨­ 1102): ").strip() or "1102"
    force = input("æ˜¯å¦å¼·åˆ¶é‡æ–°ç”Ÿæˆï¼Ÿ(y/N): ").strip().lower() == 'y'
    
    print()
    result = generate_wordcloud(int(year), company_code, force_regenerate=force)
    
    print("\n" + "=" * 50)
    if result['success']:
        if result.get('skipped'):
            print(f"â„¹ï¸ æ–‡å­—é›²å·²å­˜åœ¨: {result['output_file']}")
        else:
            print(f"âœ… æ–‡å­—é›²ç”ŸæˆæˆåŠŸ: {result['output_file']}")
        print(f"ğŸ“Š é—œéµå­—æ•¸é‡: {result['word_count']}")
        print(f"ğŸ” å‰ 10 å€‹é—œéµå­—: {', '.join(result['top_keywords'])}")
    else:
        print(f"âŒ ç”Ÿæˆå¤±æ•—: {result.get('error')}")
    print("=" * 50)


if __name__ == "__main__":
    main()
