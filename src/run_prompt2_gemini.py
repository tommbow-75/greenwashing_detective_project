import json, os, re, sys
from dotenv import load_dotenv
from google import genai
from google.genai import types

# å°å…¥é›†ä¸­é…ç½®
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from config import PATHS, DATA_FILES

# 1. è¼‰å…¥ .env æª”æ¡ˆä¸¦åˆå§‹åŒ– Client
load_dotenv()
api_key = os.getenv("GEMINI_API_KEY")

if not api_key:
    raise ValueError("æ‰¾ä¸åˆ° GEMINI_API_KEYã€‚è«‹ç¢ºä¿ .env æª”æ¡ˆå­˜åœ¨ä¸”è¨­å®šæ­£ç¢ºã€‚")

client = genai.Client(api_key=api_key)


def process_esg_news_verification(input_json_path, news_json_path, msci_json_path, output_json_path):
    """
    è™•ç† ESG æ–°èé©—è­‰
    
    Args:
        input_json_path: åŸæª”è·¯å¾‘ (2024_1102_p1.json)
        news_json_path: é©—è­‰è³‡æ–™è·¯å¾‘ (2024_1102_news.json)
        msci_json_path: MSCI åˆ¤æ–·æ¨™æº–è·¯å¾‘ (msci_flag.json)
        output_json_path: è¼¸å‡ºçµæœè·¯å¾‘
    
    Returns:
        dict: {
            'success': bool,
            'processed_items': int,
            'input_tokens': int,
            'output_tokens': int,
            'total_tokens': int,
            'api_time': float,
            'total_time': float
        }
    """

    # 2. è®€å–åŸæª”
    try:
        with open(input_json_path, 'r', encoding='utf-8') as f:
            original_data = json.load(f)
        print(f"âœ… æˆåŠŸè®€å–åŸæª”ï¼š{len(original_data)} ç­†è³‡æ–™")
    except FileNotFoundError:
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°è¼¸å…¥æª”æ¡ˆ {input_json_path}")
        return {'success': False, 'error': 'FileNotFoundError'}
    except json.JSONDecodeError:
        print(f"âŒ éŒ¯èª¤ï¼šè¼¸å…¥æª”æ¡ˆ {input_json_path} æ ¼å¼ä¸¦éæ­£ç¢ºçš„ JSON")
        return {'success': False, 'error': 'JSONDecodeError'}

    # 3. ç›´æ¥è®€å–é©—è­‰è³‡æ–™
    try:
        with open(news_json_path, 'r', encoding='utf-8') as f:
            news_data = json.load(f)
        print(f"âœ… æˆåŠŸè®€å–é©—è­‰è³‡æ–™ï¼š{len(news_data)} ç­†æ–°è")
    except Exception as e:
        print(f"âŒ éŒ¯èª¤ï¼šè®€å–é©—è­‰è³‡æ–™å¤±æ•— - {e}")
        return {'success': False, 'error': f'News data read error: {e}'}

    # 4. ä½¿ç”¨ pandas è®€å– MSCI åˆ¤æ–·æ¨™æº–
    try:
        with open(msci_json_path, 'r', encoding='utf-8') as f:
            msci_flag = json.load(f)
        print(f"âœ… æˆåŠŸè®€å– MSCI åˆ¤æ–·æ¨™æº–")
    except Exception as e:
        print(f"âŒ éŒ¯èª¤ï¼šè®€å– MSCI æ¨™æº–å¤±æ•— - {e}")
        return {'success': False, 'error': f'MSCI data read error: {e}'}

    # 5. æº–å‚™ Promptï¼ˆå°‡è®Šæ•¸åµŒå…¥ï¼‰
    prompt_template = f"""
ä½ å°‡æ‰®æ¼”ESGå¯©æŸ¥å“¡ï¼Œè² è²¬é€²è¡Œå¤–éƒ¨æ–°èæ¯”å°èˆ‡é¢¨éšªèª¿æ•´ã€‚

ã€åŸæª”èªªæ˜ã€‘
åŸæª”ç‚ºè©²å…¬å¸æ°¸çºŒå ±å‘Šæ›¸çš„è²æ˜èˆ‡é¢¨éšªåˆ†æ•¸ï¼ŒåŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š
- company: å…¬å¸åç¨±ï¼ˆä¾‹å¦‚ï¼š"äºæ³¥"ï¼‰
- company_id: å…¬å¸ä»£ç¢¼ï¼ˆä¾‹å¦‚ï¼š"1102"ï¼‰
- year: å¹´åˆ†
- esg_category: ESGåˆ†é¡ (E/S/G)
- sasb_topic: SASBä¸»é¡Œ
- page_number: é ç¢¼
- report_claim: ä¼æ¥­è²æ˜
- greenwashing_factor: æ¼‚ç¶ é¢¨éšªå› å­
- risk_score: é¢¨éšªåˆ†æ•¸
- key_word: é—œéµå­—

ã€é©—è­‰è³‡æ–™èªªæ˜ã€‘
é©—è­‰è³‡æ–™åŒ…å« {len(news_data)} ç­†æ–°èï¼Œæ¬„ä½å¦‚ä¸‹ï¼š
- news_id: æ–°èç·¨è™Ÿ
- stock_code: è‚¡ç¥¨ä»£è™Ÿ
- company_name: å…¬å¸åç¨±
- sasb_topic: SASBä¸»é¡Œ
- search_query: æœå°‹é—œéµå­—
- title: æ–°èæ¨™é¡Œ
- url: æ–°èé€£çµ
- published_date: ç™¼å¸ƒæ—¥æœŸ
- publisher: ç™¼å¸ƒè€…

ã€MSCI é¢¨éšªæ——è™Ÿåˆ¤æ–·æ¨™æº–ã€‘
  **Environment**: 
    "Red": "å¤§è¦æ¨¡ç”Ÿæ…‹æµ©åŠ«",
    "Orange": "é‡å¤§é•è¦ä½†å¯æ§",
    "Yellow": "åè¦†ç™¼ç”Ÿçš„åˆè¦å•é¡Œ",
    "Green": "ç„¡é‡å¤§è£ç½°ã€ç¬¦åˆç•¶åœ°æ³•è¦ï¼Œåƒ…æœ‰é›¶æ˜Ÿæ’æ”¾è¶…æ¨™ç´€éŒ„"
  ,
  **HumanCapital**: 
    "Red": "ç³»çµ±æ€§äººæ¬Šä¾µçŠ¯",
    "Orange": "çµæ§‹æ€§æ­§è¦–æˆ–åš´é‡è·ç½",
    "Yellow": "å–®ä¸€å‹è³‡ç³¾ç´›",
    "Green": "é›¶æ˜Ÿçš„å‹è³‡çˆ­è­°ã€ä¸€èˆ¬æ€§çš„é›¢è·ç‡æ³¢å‹•ï¼Œæˆ–å·²è§£æ±ºçš„å–®ä¸€ç½°å–®"
  ,
  **SocialCapital**: 
    "Red": "ç½é›£æ€§ç”¢å“é¢¨éšªæˆ–éš±ç§å´©æ½°",
    "Orange": "é‡å¤§ç”¢å“å¬å›æˆ–é›†é«”è¨´è¨Ÿ",
    "Yellow": "å±€éƒ¨æ€§æŠ•è¨´",
    "Green": "ä¸€èˆ¬æ€§çš„å®¢æˆ¶æœå‹™æŠ•è¨´ã€é›¶æ˜Ÿçš„é€€è²¨å•é¡Œ"
  ,
  **LeadershipAndGovernance**: 
    "Red": "æ ¸å¿ƒç³»çµ±å´©æ½°æˆ–å¤§è¦æ¨¡è²ªè…",
    "Orange": "é‡å¤§æ²»ç†ç¼ºé™·",
    "Yellow": "è¡Œæ”¿è™•åˆ†æˆ–å–®ä¸€æ¡ˆä»¶",
    "Green": "æ­£å¸¸çš„è‘£äº‹æœƒæ”¹é¸ã€å¾®å°çš„è¡Œæ”¿ç–å¤±è£œæ­£"

ã€è™•ç†é‚è¼¯ã€‘
1. é¢¨éšªèª¿æ•´é‚è¼¯ä¾ç…§ä¸Šè¿° MSCI æ¨™æº–
2. ä½¿ç”¨éˆå¼æ€è€ƒï¼Œå…ˆåˆ¤æ–·ã€Œå—å½±éŸ¿äººæ•¸ã€ã€ã€Œæ˜¯å¦æ¶‰åŠæ­»äº¡ã€ã€ã€Œæ˜¯å¦é•åæ³•è¦ã€ï¼Œæœ€å¾Œå†è¼¸å‡ºæ——è™Ÿ
3. æ‰£åˆ†æ©Ÿåˆ¶ï¼šred = -4, orange = -2, yellow = -1, green = 0
4. ç‰¹åˆ¥æ³¨æ„ã€Œæ©˜æ——ã€èˆ‡ã€Œç´…æ——ã€çš„é‚Šç•Œï¼š
   - Red é€šå¸¸æ¶‰åŠã€Œç³»çµ±æ€§ã€é•·æœŸã€ä¸å¯é€†ã€
   - Orange å‰‡å¤šç‚ºã€Œå¤§è¦æ¨¡ã€åš´é‡ã€ä½†å·²é–‹å§‹ä¿®å¾©ã€
5. å…ˆæ¯”å° sasb_topic ä¸€è‡´ï¼Œå†ä¾æ“šåŸæª” report_claim å¾é©—è­‰è³‡æ–™é¸å‡ºä¸€ç­†æœ€å…·ä»£è¡¨æ€§çš„æ–°è
6. è‹¥åŸæª”è¼¸å…¥ X ç­†è²ç¨±ï¼Œå°±è¦è¼¸å‡º X ç­†çµæœ

ã€ç›¸é—œæ€§æª¢æŸ¥ã€‘
æ¯”å°å‰ï¼Œè«‹å…ˆåŸ·è¡Œç›¸é—œæ€§æª¢æŸ¥ï¼š
- æª¢æŸ¥é©—è­‰è³‡æ–™æ˜¯å¦æ˜ç¢ºæåŠ 'company' æˆ– 'company_code'
- å¦‚æœæ˜¯åœ¨è¬›å…¶ä»–å…¬å¸ï¼Œè«‹åˆ¤å®šç‚ºç„¡æ•ˆ
- æª¢æŸ¥æ–°èå…§å®¹æ˜¯å¦èˆ‡ report_claim çš„ä¸»é¡Œæœ‰å¯¦è³ªé—œè¯ï¼Ÿ
- å¦‚æœç™¼ç¾æ–°èèˆ‡å…¬å¸ç„¡é—œã€ä¸»é¡Œå®Œå…¨ä¸ç¬¦ã€ç„¡æ–°èï¼Œè«‹ç›´æ¥è¼¸å‡ºï¼š
  * consistency_status: "ä¸€è‡´"
  * external_evidence: "ç„¡ç›¸é—œæ–°èè­‰æ“š"
  * external_evidence_url: ""
  * msci_flag: "Green"
  * adjustment_score: (ç¶­æŒåŸ risk_score)

ã€è¼¸å‡ºæ ¼å¼ã€‘
è¼¸å‡ºæ¬„ä½è¦æ±‚ (åš´æ ¼åŸ·è¡Œ)ï¼Œä¸è¦æ·»åŠ ä»»ä½•å‰è¨€ã€å¾Œèªæˆ–èªªæ˜æ–‡å­—ã€‚

é‡è¦ï¼šè«‹ä¿æŒåŸæª”æ¬„ä½åç¨±ä¸è®Šï¼Œç‰¹åˆ¥æ˜¯ï¼š
- **company** å¿…é ˆç¶­æŒåŸæª”çš„å…¬å¸åç¨±æ ¼å¼ï¼ˆä¾‹å¦‚ "äºæ³¥"ï¼‰ï¼Œä¸è¦è½‰æ›ç‚ºä»£ç¢¼
- **company_id** å¿…é ˆç¶­æŒåŸæª”çš„å…¬å¸ä»£ç¢¼ï¼ˆä¾‹å¦‚ "1102"ï¼‰
- **report_claim** æ¬„ä½åç¨±ç¶­æŒä¸è®Šï¼Œä¸è¦æ”¹ç‚º disclosure_claim

è¼¸å‡ºç¯„ä¾‹ï¼š
**company**: {original_data[0]['company']},  # å¿…é ˆæ˜¯åç¨±ï¼Œä¾‹å¦‚ "äºæ³¥"
**company_id**: {original_data[0]['company_id']},  # å¿…é ˆæ˜¯ä»£è™Ÿï¼Œä¾‹å¦‚ "1102"
**year**: {original_data[0]['year']},
**esg_category**: {original_data[0]['esg_category']},
**sasb_topic**: {original_data[0]['sasb_topic']},
**page_number**: {original_data[0]['page_number']},
**report_claim**: {original_data[0]['report_claim']},  # ç¶­æŒæ­¤æ¬„ä½åç¨±
**greenwashing_factor**: {original_data[0]['greenwashing_factor']},
**risk_score**: {original_data[0]['risk_score']},
**external_evidence**: é©—è­‰è³‡æ–™æ¨™é¡Œæˆ–'ç„¡ç›¸é—œæ–°èè­‰æ“š',
**external_evidence_url**: é©—è­‰è³‡æ–™æ–°èé€£çµæˆ–ç©ºå­—ä¸²,
**consistency_status**: ä¸€è‡´/éƒ¨åˆ†ç¬¦åˆ/ä¸ä¸€è‡´,
**msci_flag**: Green/Yellow/Orange/Red,
**adjustment_score**: èª¿æ•´å¾Œåˆ†æ•¸ï¼ˆæœ€ä½ç‚º0ï¼‰


çµ•å°ä¸è¦å¼·è¡Œå°‡ç„¡é—œçš„æ–°èé€£çµåˆ°ä¼æ¥­è²ç¨±ä¸Šã€‚
è«‹ç›´æ¥è¼¸å‡º JSON Arrayã€‚
"""

    # 6. å°‡åŸæª”å’Œé©—è­‰è³‡æ–™è½‰ç‚ºå­—ä¸²
    user_input = f"""
    ã€åŸæª”æ•¸æ“šã€‘
    {json.dumps(original_data, ensure_ascii=False, indent=2)}

    ã€é©—è­‰è³‡æ–™ã€‘
    {json.dumps(news_data, ensure_ascii=False, indent=2)}
    """

    # 7. å‘¼å« Gemini API
    print("\nğŸ”„ æ­£åœ¨å‘¼å« Gemini APIï¼Œè«‹ç¨å€™...")

    try:
        response = client.models.generate_content(
            model="gemini-2.5-pro",
            contents=user_input,
            config=types.GenerateContentConfig(
                system_instruction=prompt_template,
                temperature=0,
                response_mime_type="application/json"
            )
        )
    except Exception as e:
        print(f"âŒ API å‘¼å«å¤±æ•—: {e}")
        return {'success': False, 'error': f'API call failed: {e}'}

    print(f"âœ… Gemini API å‘¼å«å®Œæˆ")

    # 8. è™•ç†èˆ‡å„²å­˜çµæœ
    raw_text = response.text.strip()

    # 8. è™•ç†èˆ‡å„²å­˜çµæœ - ç›´æ¥æŸ¥æ‰¾ JSON é™£åˆ—
    print("\nğŸ” æ­£åœ¨è§£æ JSON å›æ‡‰...")
    
    try:
        final_json = None
        all_arrays = re.findall(r'(\[.*\])', raw_text, re.DOTALL)
        if all_arrays:
            clean_json_str = all_arrays[0]
            # è™•ç†å¯èƒ½çš„å¤šå€‹é™£åˆ—
            if "][" in clean_json_str:
                clean_json_str = clean_json_str.split("][")[0] + "]"
            elif "] [" in clean_json_str:
                clean_json_str = clean_json_str.split("] [")[0] + "]"
            
            try:
                final_json = json.loads(clean_json_str)
                print("âœ… JSON è§£ææˆåŠŸ")
            except json.JSONDecodeError as e:
                print(f"âš ï¸  JSON è§£æå¤±æ•—: {e}")
        
        # å„²å­˜çµæœ
        if final_json:
            os.makedirs(os.path.dirname(output_json_path), exist_ok=True)
            with open(output_json_path, 'w', encoding='utf-8') as f:
                json.dump(final_json, f, ensure_ascii=False, indent=2)
            print(f"âœ… æˆåŠŸï¼çµæœå·²å„²å­˜è‡³ {output_json_path}ï¼Œå…± {len(final_json)} ç­†")
        else:
            raise ValueError("ç„¡æ³•å¾å›æ‡‰ä¸­æå– JSON çµæ§‹")

    except json.JSONDecodeError as e:
        print(f"âš ï¸  JSON è§£æéŒ¯èª¤: {e}")
        print("âš ï¸  æ­£åœ¨å˜—è©¦ä¿®å¾©æ¨¡å¼...")
        try:
            # ä½¿ç”¨æ‹¬è™Ÿè¨ˆæ•¸æ³•æå–å®Œæ•´ JSON
            start = raw_text.find('[')
            if start == -1:
                raise ValueError("æ‰¾ä¸åˆ° JSON é™£åˆ—èµ·å§‹æ¨™è¨˜ '['")
            
            count = 0
            end_pos = -1
            for i in range(start, len(raw_text)):
                if raw_text[i] == '[':
                    count += 1
                elif raw_text[i] == ']':
                    count -= 1
                if count == 0:
                    end_pos = i + 1
                    break
            
            if end_pos > start:
                extreme_clean = raw_text[start:end_pos]
                final_json = json.loads(extreme_clean)
                os.makedirs(os.path.dirname(output_json_path), exist_ok=True)
                with open(output_json_path, 'w', encoding='utf-8') as f:
                    json.dump(final_json, f, ensure_ascii=False, indent=2)
                print(f"âœ… ä¿®å¾©æˆåŠŸï¼çµæœå„²å­˜è‡³ {output_json_path}ï¼Œå…± {len(final_json)} ç­†")
            else:
                raise ValueError("ç„¡æ³•æ‰¾åˆ°å®Œæ•´çš„ JSON é™£åˆ—")
        except Exception as repair_error:
            print(f"âŒ ä¿®å¾©å¤±æ•—ï¼š{repair_error}")
            # å°‡åŸå§‹å›æ‡‰å„²å­˜åˆ°æ–‡ä»¶ä»¥ä¾¿èª¿è©¦
            debug_path = output_json_path.replace('.json', '_debug_response.txt')
            with open(debug_path, 'w', encoding='utf-8') as f:
                f.write(raw_text)
            print(f"ğŸ’¾ å·²å°‡å®Œæ•´åŸå§‹å›æ‡‰å„²å­˜è‡³ï¼š{debug_path}")
    except Exception as e:
        print(f"âŒ ç™¼ç”Ÿéé æœŸéŒ¯èª¤: {e}")
        # å°‡åŸå§‹å›æ‡‰å„²å­˜åˆ°æ–‡ä»¶ä»¥ä¾¿èª¿è©¦
        debug_path = output_json_path.replace('.json', '_debug_response.txt')
        with open(debug_path, 'w', encoding='utf-8') as f:
            f.write(raw_text)
        print(f"ğŸ’¾ å·²å°‡å®Œæ•´åŸå§‹å›æ‡‰å„²å­˜è‡³ï¼š{debug_path}")

    # ===== TOKEN USAGE & TIME COST =====
    total_end_time = time.perf_counter()
    total_elapsed = total_end_time - total_start_time

    print("\n" + "="*50)
    print("ğŸ“Š Token ä½¿ç”¨çµ±è¨ˆ")
    print("="*50)
    print(f"è¼¸å…¥ Token æ•¸ : {input_token_est:,}")
    print(f"è¼¸å‡º Token æ•¸ : {output_token_est:,}")
    print(f"ç¸½è¨ˆ Token æ•¸ : {total_token_est:,}")
    print("\n" + "="*50)
    print("â±ï¸  åŸ·è¡Œæ™‚é–“çµ±è¨ˆ")
    print("="*50)
    print(f"API å‘¼å«æ™‚é–“  : {api_elapsed:.2f} ç§’")
    print(f"ç¸½åŸ·è¡Œæ™‚é–“    : {total_elapsed:.2f} ç§’")
    print("="*50)
    
    # è¿”å›çµ±è¨ˆè³‡è¨Šä¾›æ¨¡çµ„åŒ–æ¥å£ä½¿ç”¨
    return {
        'success': True,
        'processed_items': len(final_json) if final_json else 0,
        'input_tokens': input_token_est,
        'output_tokens': output_token_est,
        'total_tokens': total_token_est,
        'api_time': api_elapsed,
        'total_time': total_elapsed
    }


def verify_esg_with_news(year, company_code, force_regenerate=False):
    """
    æ¨¡çµ„åŒ–æ¥å£ï¼šåŸ·è¡Œ ESG æ–°èé©—è­‰èˆ‡è©•åˆ†èª¿æ•´
    
    Args:
        year: å ±å‘Šå¹´ä»½
        company_code: å…¬å¸ä»£ç¢¼
        force_regenerate: æ˜¯å¦å¼·åˆ¶é‡æ–°ç”Ÿæˆï¼ˆé è¨­ Falseï¼‰
    
    Returns:
        dict: {
            'success': bool,
            'message': str,
            'output_path': str,  # P2.json è·¯å¾‘
            'skipped': bool,     # æ˜¯å¦è·³éç”Ÿæˆ
            'statistics': {      # åŸ·è¡Œçµ±è¨ˆ
                'processed_items': int,
                'input_tokens': int,
                'output_tokens': int,
                'total_tokens': int,
                'api_time': float,
                'total_time': float
            },
            'error': str  # éŒ¯èª¤è¨Šæ¯ï¼ˆè‹¥å¤±æ•—ï¼‰
        }
    """
    import time
    start_time = time.perf_counter()
    
    try:
        # 1. è‡ªå‹•æ§‹å»ºæª”æ¡ˆè·¯å¾‘
        base_filename = f"{year}_{company_code}"
        
        input_path = os.path.join(PATHS['P1_JSON'], f'{base_filename}_p1.json')
        news_path = os.path.join(PATHS['NEWS_SEARCH_OUTPUT'], f'{base_filename}_news.json')
        msci_path = DATA_FILES['MSCI_FLAG']
        output_path = os.path.join(PATHS['P2_JSON'], f'{base_filename}_p2.json')
        
        # 2. æª”æ¡ˆå­˜åœ¨æ€§æª¢æŸ¥ï¼ˆè·³éé‡è¤‡åŸ·è¡Œï¼‰
        if os.path.exists(output_path) and not force_regenerate:
            # è®€å–å·²å­˜åœ¨çš„æª”æ¡ˆä»¥ç²å–çµ±è¨ˆè³‡è¨Š
            try:
                with open(output_path, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                
                return {
                    'success': True,
                    'message': 'AI é©—è­‰çµæœå·²å­˜åœ¨ï¼Œè·³éç”Ÿæˆ',
                    'output_path': output_path,
                    'skipped': True,
                    'statistics': {
                        'processed_items': len(existing_data),
                        'input_tokens': 0,
                        'output_tokens': 0,
                        'total_tokens': 0,
                        'api_time': 0,
                        'total_time': time.perf_counter() - start_time
                    }
                }
            except Exception as e:
                # æª”æ¡ˆå­˜åœ¨ä½†ç„¡æ³•è®€å–ï¼Œåˆªé™¤ä¸¦é‡æ–°ç”Ÿæˆ
                print(f"âš ï¸ ç¾æœ‰æª”æ¡ˆç„¡æ³•è®€å–ï¼Œå°‡é‡æ–°ç”Ÿæˆ: {e}")
                os.remove(output_path)
        
        # 3. æª¢æŸ¥å¿…è¦è¼¸å…¥æª”æ¡ˆ
        missing_files = []
        if not os.path.exists(input_path):
            missing_files.append(f"P1 æª”æ¡ˆ: {input_path}")
        if not os.path.exists(news_path):
            missing_files.append(f"æ–°èæª”æ¡ˆ: {news_path}")
        if not os.path.exists(msci_path):
            missing_files.append(f"MSCI æ¨™æº–: {msci_path}")
        
        if missing_files:
            return {
                'success': False,
                'message': 'ç¼ºå°‘å¿…è¦è¼¸å…¥æª”æ¡ˆ',
                'error': ', '.join(missing_files),
                'output_path': None,
                'skipped': False
            }
        
        # 4. åŸ·è¡Œ AI é©—è­‰ï¼ˆç²å–çµ±è¨ˆè³‡è¨Šï¼‰
        print(f"\n{'='*60}")
        print(f"é–‹å§‹ AI é©—è­‰èˆ‡è©•åˆ†èª¿æ•´: {year} å¹´ {company_code}")
        print(f"{'='*60}")
        
        # å‘¼å«åŸæœ‰å‡½æ•¸ä¸¦ç²å–çµ±è¨ˆè³‡è¨Š
        stats = process_esg_news_verification(input_path, news_path, msci_path, output_path)
        
        # æª¢æŸ¥åŸ·è¡Œçµæœ
        if not stats or not stats.get('success'):
            error_msg = stats.get('error', 'Unknown error') if stats else 'Function returned None'
            return {
                'success': False,
                'message': 'AI é©—è­‰åŸ·è¡Œå¤±æ•—',
                'error': error_msg,
                'output_path': output_path,
                'skipped': False
            }
        
        total_time = time.perf_counter() - start_time
        
        # 5. é©—è­‰è¼¸å‡ºæª”æ¡ˆ
        if not os.path.exists(output_path):
            return {
                'success': False,
                'message': 'AI é©—è­‰åŸ·è¡Œå®Œæˆä½†æœªç”¢ç”Ÿè¼¸å‡ºæª”æ¡ˆ',
                'error': 'è¼¸å‡ºæª”æ¡ˆä¸å­˜åœ¨',
                'output_path': output_path,
                'skipped': False
            }
        
        # 6. è¿”å›çµæœï¼ˆä½¿ç”¨å¾ process_esg_news_verification ç²å¾—çš„çµ±è¨ˆè³‡è¨Šï¼‰
        return {
            'success': True,
            'message': 'AI é©—è­‰å®Œæˆ',
            'output_path': output_path,
            'skipped': False,
            'statistics': {
                'processed_items': stats.get('processed_items', 0),
                'input_tokens': stats.get('input_tokens', 0),
                'output_tokens': stats.get('output_tokens', 0),
                'total_tokens': stats.get('total_tokens', 0),
                'api_time': stats.get('api_time', 0),
                'total_time': total_time
            }
        }
    
    except FileNotFoundError as e:
        return {
            'success': False,
            'message': 'æª”æ¡ˆä¸å­˜åœ¨',
            'error': str(e),
            'output_path': None,
            'skipped': False
        }
    except json.JSONDecodeError as e:
        return {
            'success': False,
            'message': 'JSON è§£æå¤±æ•—',
            'error': str(e),
            'output_path': None,
            'skipped': False
        }
    except Exception as e:
        return {
            'success': False,
            'message': 'AI é©—è­‰åŸ·è¡Œå¤±æ•—',
            'error': str(e),
            'output_path': None,
            'skipped': False
        }


if __name__ == "__main__":
    year = "2024"
    company = "1102"
    # è¨­å®šæª”æ¡ˆè·¯å¾‘
    input_path = os.path.join(PATHS['P1_JSON'], f'{year}_{company}_p1.json')
    news_path = os.path.join(PATHS['NEWS_SEARCH_OUTPUT'], f'{year}_{company}_news.json')
    msci_path = DATA_FILES['MSCI_FLAG']
    output_path = os.path.join(PATHS['P2_JSON'], f'{year}_{company}_p2.json')
    
    process_esg_news_verification(input_path, news_path, msci_path, output_path)
