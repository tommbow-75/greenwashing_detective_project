import json
import time
import os
from datetime import datetime
from gnews import GNews
from dateutil import parser as date_parser

# --- 1. è¨­å®šæª”æ¡ˆè·¯å¾‘ ---
P1_JSON_PATH = '2024_1102_P1.json'
SASB_KEYWORD_PATH = 'sasb_keyword.json'
OUTPUT_DIR = './news_output/'

# --- 2. è‚¡ç¥¨ä»£ç¢¼å°ç…§è¡¨ ---
# æ ¹æ“šå¯¦éš›éœ€æ±‚æ“´å……
stock_map = {
    "1102": "äºæ³¥",
    "1101": "å°æ³¥",
    "2330": "å°ç©é›»",
    "2317": "é´»æµ·",
}

# --- 3. è®€å–è¼¸å…¥æª”æ¡ˆ ---
print("æ­£åœ¨è®€å–è¼¸å…¥æª”æ¡ˆ...")

# è®€å– P1.json
with open(P1_JSON_PATH, 'r', encoding='utf-8') as f:
    p1_data_list = json.load(f)
print(f"âœ“ å·²è¼‰å…¥ {len(p1_data_list)} ç­† P1 è³‡æ–™")

# è®€å– SASB é—œéµå­—å°ç…§è¡¨
with open(SASB_KEYWORD_PATH, 'r', encoding='utf-8') as f:
    sasb_keywords_map = json.load(f)
print(f"âœ“ å·²è¼‰å…¥ {len(sasb_keywords_map)} å€‹ SASB è­°é¡Œé—œéµå­—")

# --- 4. å»ºç«‹è¼¸å‡ºç›®éŒ„ ---
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)
    print(f"âœ“ å·²å»ºç«‹è¼¸å‡ºç›®éŒ„: {OUTPUT_DIR}")

# --- 5. æ—¥æœŸé©—è­‰å‡½æ•¸ ---
def is_date_in_year(date_str, target_year):
    """
    æª¢æŸ¥æ–°èç™¼å¸ƒæ—¥æœŸæ˜¯å¦åœ¨ç›®æ¨™å¹´ä»½å…§
    :param date_str: æ–°èç™¼å¸ƒæ—¥æœŸå­—ä¸²
    :param target_year: ç›®æ¨™å¹´ä»½ (int)
    :return: True å¦‚æœåœ¨ç›®æ¨™å¹´ä»½å…§ï¼Œå¦å‰‡ False
    """
    if not date_str:
        return False
    
    try:
        # è§£ææ—¥æœŸå­—ä¸²
        parsed_date = date_parser.parse(date_str)
        # æª¢æŸ¥å¹´ä»½æ˜¯å¦åŒ¹é…
        return parsed_date.year == target_year
    except Exception as e:
        # å¦‚æœç„¡æ³•è§£ææ—¥æœŸï¼Œä¿å®ˆèµ·è¦‹æ’é™¤è©²æ–°è
        print(f"  âš ï¸  ç„¡æ³•è§£ææ—¥æœŸ: {date_str}")
        return False

# --- 6. åˆå§‹åŒ–çµæœå®¹å™¨ ---
# ç°¡åŒ–è¼¸å‡ºæ ¼å¼ï¼šåªä¿ç•™å¿…è¦æ¬„ä½
all_news_articles = []
news_id_counter = 1

# --- 6. åŸ·è¡Œæœå°‹ä¸»ç¨‹åº ---
print(f"\né–‹å§‹åŸ·è¡Œå¤–éƒ¨æŸ¥æ ¸ï¼Œå…± {len(p1_data_list)} ç­†è³‡æ–™...\n")
print("=" * 60)

for idx, item in enumerate(p1_data_list, 1):
    # å–å¾—åŸºæœ¬è³‡è¨Š
    stock_code = item.get("company")
    company_name = stock_map.get(stock_code, stock_code)
    topic = item.get("sasb_topic")
    year_str = item.get("year")
    esg_category = item.get("esg_category")
    risk_score = item.get("risk_score")
    report_claim = item.get("report_claim", "")
    
    print(f"[{idx}/{len(p1_data_list)}] æŸ¥æ ¸: {company_name} - {topic} ({year_str})")
    
    # è¨­å®š GNews æ™‚é–“ç¯„åœ
    try:
        target_year = int(year_str)
        # æœç´¢ç¯„åœé™åˆ¶åœ¨ f"{year}0101~{year}1231"
        google_news = GNews(language='zh-TW', country='TW', max_results=5)
        google_news.start_date = (target_year, 1, 1)
        google_news.end_date = (target_year, 12, 31)
        date_range = f"{target_year}0101~{target_year}1231"
        print(f"  ğŸ“… æœç´¢ç¯„åœ: {date_range}")
    except ValueError:
        print(f"  âš ï¸  æ—¥æœŸæ ¼å¼éŒ¯èª¤ï¼Œè·³éæ­¤ç­†")
        search_results["results"].append(result_item)
        continue
    
    # æŸ¥æ‰¾å°æ‡‰çš„é—œéµå­—
    keywords = None
    
    # å…ˆå˜—è©¦å®Œå…¨åŒ¹é…
    if topic in sasb_keywords_map:
        keywords = sasb_keywords_map[topic]
    else:
        # å˜—è©¦æ¨¡ç³ŠåŒ¹é…ï¼ˆè™•ç†åç¨±å·®ç•°ï¼‰
        for key in sasb_keywords_map.keys():
            if topic in key or key in topic:
                keywords = sasb_keywords_map[key]
                print(f"  â„¹ï¸  ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…: '{topic}' -> '{key}'")
                break
    
    if keywords:
        # çµ„åˆæœå°‹å­—ä¸²
        keyword_str = " OR ".join(keywords)
        query = f'{company_name} ({keyword_str})'
        
        print(f"  ğŸ” æœå°‹: {query[:60]}...")
        
        try:
            # åŸ·è¡Œæœå°‹
            news_results = google_news.get_news(query)
            
            if news_results:
                # éæ¿¾ï¼šåªä¿ç•™ç›®æ¨™å¹´ä»½å…§çš„æ–°è
                filtered_news = []
                filtered_out_count = 0
                
                for news in news_results:
                    published_date = news.get('published date', '')
                    
                    # æª¢æŸ¥æ—¥æœŸæ˜¯å¦åœ¨ç›®æ¨™å¹´ä»½å…§
                    if is_date_in_year(published_date, target_year):
                        # æ·»åŠ åˆ°ç¸½çµæœåˆ—è¡¨ï¼ŒåŒ…å«æµæ°´è™Ÿ
                        all_news_articles.append({
                            "news_id": news_id_counter,
                            "sasb_topic": topic,
                            "title": news.get('title', ''),
                            "url": news.get('url', ''),
                            "published_date": published_date,
                            "publisher": news.get('publisher', {}).get('title', '') if isinstance(news.get('publisher'), dict) else ''
                        })
                        news_id_counter += 1
                        filtered_news.append(news)  # ç”¨æ–¼è¨ˆæ•¸
                    else:
                        filtered_out_count += 1
                
                if filtered_news:
                    print(f"  âœ“ æ‰¾åˆ° {len(news_results)} å‰‡æ–°èï¼Œéæ¿¾å¾Œä¿ç•™ {len(filtered_news)} å‰‡ï¼ˆæ’é™¤ {filtered_out_count} å‰‡é{target_year}å¹´æ–°èï¼‰")
                else:
                    print(f"  â„¹ï¸  æ‰¾åˆ° {len(news_results)} å‰‡æ–°èï¼Œä½†å…¨éƒ¨ä¸åœ¨ {target_year} å¹´ç¯„åœå…§")
            else:
                print("  â„¹ï¸  ç„¡ç›¸é—œæ–°è")
                
        except Exception as e:
            print(f"  âŒ æœå°‹å¤±æ•—: {str(e)}")
        
        # é¿å…è«‹æ±‚å¤ªå¿«è¢«æ“‹
        time.sleep(2)
        
    else:
        print(f"  âš ï¸  è­¦å‘Š: è­°é¡Œ '{topic}' æœªåœ¨é—œéµå­—å°ç…§è¡¨ä¸­æ‰¾åˆ°")
    
    print("-" * 60)

# --- 7. å„²å­˜çµæœ ---
output_filename = os.path.join(OUTPUT_DIR, f'news_results_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json')

with open(output_filename, 'w', encoding='utf-8') as f:
    json.dump(all_news_articles, f, ensure_ascii=False, indent=2)

print("\n" + "=" * 60)
print(f"âœ… æŸ¥æ ¸å®Œæˆï¼")
print(f"ğŸ“ çµæœå·²å„²å­˜è‡³: {output_filename}")
print(f"ğŸ“Š çµ±è¨ˆ:")
print(f"   - ç¸½æŸ¥æ ¸ç­†æ•¸: {len(p1_data_list)}")
print(f"   - æ–°èç¸½æ•¸: {len(all_news_articles)}")
print("=" * 60)