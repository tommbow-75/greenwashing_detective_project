"""
æ–°èçˆ¬èŸ²æ¨¡çµ„

æä¾›å¾ P1 JSON åˆ†æçµæœæœå°‹ç›¸é—œæ–°èçš„åŠŸèƒ½ã€‚

ä¸»è¦å‡½æ•¸ï¼š
    search_news_for_report: é‡å° ESG å ±å‘Šæœå°‹ç›¸é—œæ–°è

ä½¿ç”¨ç¯„ä¾‹ï¼š
    from news_search.crawler_news import search_news_for_report
    
    result = search_news_for_report(year=2024, company_code="1102")
    if result['success']:
        print(f"æ‰¾åˆ° {result['news_count']} å‰‡æ–°è")
"""

import json
import time
import os
from datetime import datetime
from typing import Dict, List, Any, Optional
from gnews import GNews
from dateutil import parser as date_parser

# === æ¨¡çµ„å¸¸æ•¸ ===
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
DEFAULT_P1_DIR = os.path.abspath(os.path.join(SCRIPT_DIR, "..", "temp_data", "prompt1_json"))
DEFAULT_OUTPUT_DIR = os.path.join(SCRIPT_DIR, "news_output")
COMPANY_MAP_PATH = os.path.abspath(os.path.join(SCRIPT_DIR, "..", "static", "data", "tw_listed_companies.json"))
SASB_KEYWORD_PATH = os.path.join(SCRIPT_DIR, "sasb_keyword.json")

# API è¨­å®š
MAX_RETRIES = 3
RETRY_DELAY = 5  # ç§’
SEARCH_DELAY = 2  # æ¯æ¬¡æœå°‹å¾Œå»¶é²
MAX_RESULTS_PER_TOPIC = 10


# === è¼”åŠ©å‡½æ•¸ ===

def _load_company_map() -> Dict[str, str]:
    """
    è¼‰å…¥ä¸Šå¸‚å…¬å¸ä»£è™Ÿå°ç…§è¡¨
    
    Returns:
        Dict[å…¬å¸ä»£è™Ÿ, å…¬å¸åç¨±]
    """
    try:
        with open(COMPANY_MAP_PATH, 'r', encoding='utf-8') as f:
            companies = json.load(f)
            stock_map = {}
            for company in companies:
                code = company.get('å…¬å¸ä»£è™Ÿ')
                name = company.get('å…¬å¸ç°¡ç¨±', company.get('å…¬å¸åç¨±', ''))
                if code:
                    stock_map[code] = name
            return stock_map
    except FileNotFoundError:
        print(f"âš ï¸ æ‰¾ä¸åˆ°å…¬å¸å°ç…§è¡¨: {COMPANY_MAP_PATH}")
        return {}
    except Exception as e:
        print(f"âš ï¸ è¼‰å…¥å…¬å¸å°ç…§è¡¨å¤±æ•—: {e}")
        return {}


def _load_sasb_keywords() -> Dict[str, List[str]]:
    """
    è¼‰å…¥ SASB è­°é¡Œé—œéµå­—å°ç…§è¡¨
    
    Returns:
        Dict[SASBè­°é¡Œ, é—œéµå­—åˆ—è¡¨]
    """
    try:
        with open(SASB_KEYWORD_PATH, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"âš ï¸ æ‰¾ä¸åˆ° SASB é—œéµå­—æª”æ¡ˆ: {SASB_KEYWORD_PATH}")
        return {}
    except Exception as e:
        print(f"âš ï¸ è¼‰å…¥ SASB é—œéµå­—å¤±æ•—: {e}")
        return {}


def _get_keywords_from_sasb(sasb_topic: str, company_name: str, sasb_keywords: Dict) -> str:
    """
    å¾ SASB é—œéµå­—è¡¨ç”Ÿæˆæœå°‹é—œéµå­—
    
    Args:
        sasb_topic: SASB è­°é¡Œåç¨±
        company_name: å…¬å¸åç¨±
        sasb_keywords: SASB é—œéµå­—å­—å…¸
    
    Returns:
        çµ„åˆå¾Œçš„æœå°‹é—œéµå­—
    """
    keywords = sasb_keywords.get(sasb_topic, [])
    if keywords:
        # å–å‰ 3 å€‹é—œéµå­—
        selected_keywords = ' '.join(keywords[:3])
        return f"{company_name} {sasb_topic} {selected_keywords}"
    else:
        return f"{company_name} {sasb_topic}"


def _find_p1_json(year: int, company_code: str, p1_dir: str = DEFAULT_P1_DIR) -> Optional[str]:
    """
    å°‹æ‰¾ P1 JSON æª”æ¡ˆ
    
    Args:
        year: å¹´ä»½
        company_code: å…¬å¸ä»£ç¢¼
        p1_dir: P1 JSON ç›®éŒ„
    
    Returns:
        P1 JSON æª”æ¡ˆè·¯å¾‘ï¼Œè‹¥æ‰¾ä¸åˆ°å‰‡è¿”å› None
    """
    # å˜—è©¦æ¨™æº–æª”åæ ¼å¼
    standard_name = f"{year}_{company_code}_p1.json"
    standard_path = os.path.join(p1_dir, standard_name)
    
    if os.path.exists(standard_path):
        return standard_path
    
    # å˜—è©¦å°å¯« p1
    lowercase_name = f"{year}_{company_code}_p1.json"
    lowercase_path = os.path.join(p1_dir, lowercase_name)
    
    if os.path.exists(lowercase_path):
        return lowercase_path
    
    # å˜—è©¦æœå°‹ç¬¦åˆæ ¼å¼çš„æª”æ¡ˆ
    if os.path.exists(p1_dir):
        prefix = f"{year}_{company_code}"
        for filename in os.listdir(p1_dir):
            if filename.startswith(prefix) and filename.lower().endswith('.json'):
                return os.path.join(p1_dir, filename)
    
    return None


def _is_date_in_year(date_str: str, target_year: int) -> bool:
    """
    æª¢æŸ¥æ–°èç™¼å¸ƒæ—¥æœŸæ˜¯å¦åœ¨ç›®æ¨™å¹´ä»½å…§
    
    Args:
        date_str: æ–°èç™¼å¸ƒæ—¥æœŸå­—ä¸²
        target_year: ç›®æ¨™å¹´ä»½
    
    Returns:
        True å¦‚æœåœ¨ç›®æ¨™å¹´ä»½å…§ï¼Œå¦å‰‡ False
    """
    if not date_str:
        return False
    
    try:
        parsed_date = date_parser.parse(date_str)
        return parsed_date.year == target_year
    except Exception:
        return False


# === ä¸»è¦å‡½æ•¸ ===

def search_news_for_report(
    year: int,
    company_code: str,
    p1_json_path: Optional[str] = None,
    force_regenerate: bool = False
) -> Dict[str, Any]:
    """
    é‡å° ESG å ±å‘Šæœå°‹ç›¸é—œæ–°è
    
    Args:
        year: å ±å‘Šå¹´ä»½
        company_code: å…¬å¸ä»£ç¢¼
        p1_json_path: P1 JSON è·¯å¾‘ï¼ˆé¸å¡«ï¼Œé è¨­è‡ªå‹•å°‹æ‰¾ï¼‰
        force_regenerate: æ˜¯å¦å¼·åˆ¶é‡æ–°ç”Ÿæˆï¼ˆé è¨­ Falseï¼‰
    
    Returns:
        {
            'success': bool,              # æ˜¯å¦æˆåŠŸ
            'output_file': str,           # è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
            'news_count': int,            # æ–°èç¸½æ•¸
            'processed_items': int,       # è™•ç†çš„ P1 é …ç›®æ•¸
            'failed_items': int,          # æœå°‹å¤±æ•—çš„é …ç›®æ•¸
            'skipped': bool,              # æ˜¯å¦è·³éç”Ÿæˆ
            'error': str                  # éŒ¯èª¤è¨Šæ¯ï¼ˆå¯é¸ï¼‰
        }
    """
    start_time = time.time()
    
    # === 1. å»ºç«‹è¼¸å‡ºç›®éŒ„ ===
    if not os.path.exists(DEFAULT_OUTPUT_DIR):
        os.makedirs(DEFAULT_OUTPUT_DIR)
    
    # è¼¸å‡ºæª”åï¼ˆç„¡æ™‚é–“æˆ³ï¼‰
    output_filename = os.path.join(DEFAULT_OUTPUT_DIR, f"{year}_{company_code}_news.json")
    
    # === 2. æª”æ¡ˆå­˜åœ¨æ€§æª¢æŸ¥ ===
    if not force_regenerate and os.path.exists(output_filename):
        try:
            with open(output_filename, 'r', encoding='utf-8') as f:
                existing_data = json.load(f)
            
            if isinstance(existing_data, list) and len(existing_data) > 0:
                return {
                    'success': True,
                    'output_file': output_filename,
                    'news_count': len(existing_data),
                    'processed_items': 0,
                    'failed_items': 0,
                    'skipped': True
                }
        except (json.JSONDecodeError, IOError):
            print(f"âš ï¸ ç¾æœ‰æª”æ¡ˆæ ¼å¼éŒ¯èª¤ï¼Œå°‡é‡æ–°ç”Ÿæˆ")
    
    # === 3. å°‹æ‰¾ P1 JSON ===
    if p1_json_path is None:
        p1_json_path = _find_p1_json(year, company_code)
    
    if p1_json_path is None or not os.path.exists(p1_json_path):
        return {
            'success': False,
            'error': f'æ‰¾ä¸åˆ° P1 JSON æª”æ¡ˆ: {year}_{company_code}_p1.json'
        }
    
    # === 4. è¼‰å…¥è³‡æº ===
    try:
        with open(p1_json_path, 'r', encoding='utf-8') as f:
            p1_data_list = json.load(f)
    except Exception as e:
        return {
            'success': False,
            'error': f'è®€å– P1 JSON å¤±æ•—: {str(e)}'
        }
    
    stock_map = _load_company_map()
    sasb_keywords = _load_sasb_keywords()
    
    # === 5. åŸ·è¡Œæ–°èæœå°‹ ===
    all_news_articles = []
    news_id_counter = 1
    processed_items = 0
    failed_items = 0
    failure_details = []
    
    print(f"\né–‹å§‹åŸ·è¡Œæ–°èæœå°‹ï¼Œå…± {len(p1_data_list)} ç­†è³‡æ–™...")
    print("=" * 60)
    
    for idx, item in enumerate(p1_data_list, 1):
        # å–å¾—åŸºæœ¬è³‡è¨Š
        company_name = item.get("company", "")  # ç¾åœ¨ç›´æ¥æ˜¯å…¬å¸åç¨±
        stock_code = item.get("company_id", company_code)  # å¾ company_id å–å¾—ä»£ç¢¼
        topic = item.get("sasb_topic", "")
        year_str = item.get("year", str(year))
        
        print(f"[{idx}/{len(p1_data_list)}] æŸ¥æ ¸: {company_name} ({stock_code}) - {topic}")
        
        processed_items += 1
        
        # === é—œéµå­—ä¸‰å±¤ç´š Fallback ===
        # å±¤ç´š 1: å„ªå…ˆä½¿ç”¨ P1 æä¾›çš„ key_word
        key_word = item.get("key_word", "")
        
        # å±¤ç´š 2: å¾ SASB é—œéµå­—è¡¨ç”Ÿæˆ
        if not key_word and topic:
            key_word = _get_keywords_from_sasb(topic, company_name, sasb_keywords)
            print(f"  ğŸ”§ ä½¿ç”¨ SASB é—œéµå­—ç”Ÿæˆ: {key_word}")
        
        # å±¤ç´š 3: åŸºæœ¬çµ„åˆ
        if not key_word:
            key_word = f"{company_name} {topic}"
            print(f"  ğŸ”§ ä½¿ç”¨åŸºæœ¬çµ„åˆ: {key_word}")
        
        # è¨­å®š GNews æ™‚é–“ç¯„åœ
        try:
            target_year = int(year_str)
            google_news = GNews(language='zh-TW', country='TW', max_results=MAX_RESULTS_PER_TOPIC)
            google_news.start_date = (target_year, 1, 1)
            google_news.end_date = (target_year, 12, 31)
            print(f"  ğŸ“… æœç´¢ç¯„åœ: {target_year}/01/01 ~ {target_year}/12/31")
        except ValueError:
            print(f"  âš ï¸ æ—¥æœŸæ ¼å¼éŒ¯èª¤ï¼Œè·³éæ­¤ç­†")
            failed_items += 1
            failure_details.append({'topic': topic, 'reason': 'æ—¥æœŸæ ¼å¼éŒ¯èª¤'})
            continue
        
        # === æœå°‹ç­–ç•¥ï¼ˆä¸‰éšæ®µï¼‰ ===
        news_results = None
        final_query = key_word
        
        try:
            # ç­–ç•¥ 1: ä½¿ç”¨å®Œæ•´é—œéµå­—
            print(f"  ğŸ” æœå°‹ç­–ç•¥1: {key_word}")
            for attempt in range(MAX_RETRIES):
                try:
                    news_results = google_news.get_news(key_word)
                    break
                except Exception as e:
                    if attempt < MAX_RETRIES - 1:
                        print(f"  âš ï¸ æœå°‹å¤±æ•—ï¼Œ{RETRY_DELAY}ç§’å¾Œé‡è©¦...")
                        time.sleep(RETRY_DELAY)
                    else:
                        raise e
            
            # ç­–ç•¥ 2: ç°¡åŒ–é—œéµå­—ï¼ˆå–å‰3å€‹è©ï¼‰
            if not news_results or len(news_results) < 3:
                key_words_list = key_word.split()
                if len(key_words_list) >= 3:
                    query2 = ' '.join(key_words_list[:3])
                    print(f"  ğŸ” æœå°‹ç­–ç•¥2: {query2}")
                    news_results2 = google_news.get_news(query2)
                    if news_results2 and len(news_results2) > len(news_results or []):
                        news_results = news_results2
                        final_query = query2
            
            # ç­–ç•¥ 3: å…¬å¸åç¨± + ä¸»é¡Œ
            if not news_results or len(news_results) < 2:
                query3 = f"{company_name} {topic}"
                print(f"  ğŸ” æœå°‹ç­–ç•¥3: {query3}")
                news_results3 = google_news.get_news(query3)
                if news_results3 and len(news_results3) > len(news_results or []):
                    news_results = news_results3
                    final_query = query3
            
            # éæ¿¾æ–°è
            if news_results:
                print(f"  ğŸ“° å…±æ‰¾åˆ° {len(news_results)} å‰‡æ–°èï¼Œé–‹å§‹éæ¿¾...")
                filtered_count = 0
                filtered_out_count = 0
                
                for news in news_results:
                    published_date = news.get('published date', '')
                    
                    if _is_date_in_year(published_date, target_year):
                        all_news_articles.append({
                            "news_id": news_id_counter,
                            "stock_code": stock_code,
                            "company_name": company_name,
                            "sasb_topic": topic,
                            "search_query": final_query,
                            "title": news.get('title', ''),
                            "url": news.get('url', ''),
                            "published_date": published_date,
                            "publisher": news.get('publisher', {}).get('title', '') if isinstance(news.get('publisher'), dict) else ''
                        })
                        news_id_counter += 1
                        filtered_count += 1
                    else:
                        filtered_out_count += 1
                
                if filtered_count > 0:
                    print(f"  âœ“ ä¿ç•™ {filtered_count} å‰‡ {target_year} å¹´æ–°èï¼ˆæ’é™¤ {filtered_out_count} å‰‡ï¼‰")
                else:
                    print(f"  âš ï¸ æ‰¾åˆ° {len(news_results)} å‰‡æ–°èï¼Œä½†å…¨éƒ¨ä¸åœ¨ {target_year} å¹´ç¯„åœå…§")
            else:
                print(f"  âŒ ç„¡ç›¸é—œæ–°è")
                
        except Exception as e:
            print(f"  âŒ æœå°‹å¤±æ•—: {str(e)}")
            failed_items += 1
            failure_details.append({'topic': topic, 'reason': str(e)})
        
        # é¿å…è«‹æ±‚å¤ªå¿«
        time.sleep(SEARCH_DELAY)
        print("-" * 60)
    
    # === 6. å„²å­˜çµæœ ===
    try:
        with open(output_filename, 'w', encoding='utf-8') as f:
            json.dump(all_news_articles, f, ensure_ascii=False, indent=2)
        
        elapsed_time = time.time() - start_time
        
        print("\n" + "=" * 60)
        print(f"âœ… æ–°èæœå°‹å®Œæˆï¼")
        print(f"ğŸ“ çµæœå·²å„²å­˜è‡³: {output_filename}")
        print(f"ğŸ“Š çµ±è¨ˆ:")
        print(f"   - è™•ç†é …ç›®æ•¸: {processed_items}")
        print(f"   - æˆåŠŸé …ç›®æ•¸: {processed_items - failed_items}")
        print(f"   - å¤±æ•—é …ç›®æ•¸: {failed_items}")
        print(f"   - æ–°èç¸½æ•¸: {len(all_news_articles)}")
        print(f"   - åŸ·è¡Œæ™‚é–“: {elapsed_time:.1f} ç§’")
        print("=" * 60)
        
        return {
            'success': True,
            'output_file': output_filename,
            'news_count': len(all_news_articles),
            'processed_items': processed_items,
            'failed_items': failed_items,
            'skipped': False,
            'failure_details': failure_details if failure_details else None
        }
        
    except Exception as e:
        return {
            'success': False,
            'error': f'å„²å­˜æª”æ¡ˆå¤±æ•—: {str(e)}'
        }


# === å‘½ä»¤åˆ—åŸ·è¡Œ ===

def main():
    """å‘½ä»¤åˆ—åŸ·è¡Œçš„ä¸»å‡½æ•¸"""
    print("=== ESG å ±å‘Šæ›¸æ–°èçˆ¬èŸ² ===\n")
    
    year = input("è«‹è¼¸å…¥å¹´ä»½ (é è¨­ 2024): ").strip() or "2024"
    company_code = input("è«‹è¼¸å…¥å…¬å¸ä»£ç¢¼ (é è¨­ 1102): ").strip() or "1102"
    force = input("æ˜¯å¦å¼·åˆ¶é‡æ–°ç”Ÿæˆï¼Ÿ(y/N): ").strip().lower() == 'y'
    
    result = search_news_for_report(
        year=int(year),
        company_code=company_code,
        force_regenerate=force
    )
    
    if result['success']:
        if result.get('skipped'):
            print(f"\nâ„¹ï¸ æ–°èè³‡æ–™å·²å­˜åœ¨ï¼Œè·³éç”Ÿæˆ")
            print(f"ğŸ“ æª”æ¡ˆä½ç½®: {result['output_file']}")
            print(f"ğŸ“Š æ–°èæ•¸é‡: {result['news_count']}")
        else:
            print(f"\nâœ… åŸ·è¡ŒæˆåŠŸï¼")
    else:
        print(f"\nâŒ åŸ·è¡Œå¤±æ•—: {result.get('error')}")


if __name__ == "__main__":
    main()